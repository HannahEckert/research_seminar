{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda, Concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import spambase\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the Spambase dataset\n",
    "data = spambase.load_data()\n",
    "X, y = data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding for classification task\n",
    "num_classes = 2  # Assuming binary classification for Spambase dataset\n",
    "y_train_onehot = to_categorical(y_train, num_classes)\n",
    "y_test_onehot = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Set up the VAE architecture\n",
    "original_dim = X_train.shape[1]\n",
    "latent_dim = 2\n",
    "intermediate_dim = 256\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "\n",
    "# Latent space\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "# Sampling function\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# Classifier\n",
    "classifier_h = Dense(128, activation='relu')(z)\n",
    "classifier_output = Dense(num_classes, activation='softmax')(classifier_h)\n",
    "\n",
    "# Decoder\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# End-to-end VAE model\n",
    "vae = Model(inputs, [x_decoded_mean, classifier_output])\n",
    "\n",
    "# Define the VAE loss\n",
    "xent_loss = original_dim * objectives.binary_crossentropy(inputs, x_decoded_mean)\n",
    "kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "classification_loss = K.categorical_crossentropy(y_train_onehot, classifier_output)\n",
    "\n",
    "vae_loss = K.mean(xent_loss + kl_loss + classification_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "\n",
    "# Train the VAE model\n",
    "vae.fit(X_train, epochs=50, batch_size=64, validation_data=(X_test, [X_test, y_test_onehot]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
